{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET PARAMETERS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "main params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize params\n",
    "DEBUG = True\n",
    "saveReport = True\n",
    "toPrint = True\n",
    "txt_label = \"Classification of integrated c1 and c2 CNVs samples\"\n",
    "split_with_stratify = False\n",
    "\n",
    "to_compute_euclidean_distances = True # if you run it again you can set it to False\n",
    "to_save_euclidean_distances = True # this only matter when to_compute is True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature selection filters (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature selection: defaults are None\n",
    "# if not then abs_mean_filter has priority over topN if both not None\n",
    "abs_mean_filter = None   # default\n",
    "topN = None              # default\n",
    "# abs_mean_filter = 0.001  # nnz\n",
    "# abs_mean_filter = 0.1    # nnz_0.1\n",
    "abs_mean_filter = 0.2    # nnz_0.2\n",
    "# topN = 10                # top10\n",
    "# topN = 5                 # top5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "generate sub-folder name for output files according to feature selection filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if split_with_stratify:\n",
    "    reportName = \"stratify_split/\"\n",
    "else:\n",
    "    reportName = \"balanced_split/\"\n",
    "    \n",
    "## define the sub-folder name of the output\n",
    "if abs_mean_filter is None and topN is None:\n",
    "    reportName = reportName+'all_features'\n",
    "else:\n",
    "    if abs_mean_filter is not None:\n",
    "        reportName = reportName+'nnz'+str(abs_mean_filter)+'_features'\n",
    "    else:\n",
    "        reportName = reportName+'top'+str(topN)+'_features'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "classification params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train test split params\n",
    "split_train_size = 40 # samples to train the classification\n",
    "split_random_state = 0 # for reproducibility\n",
    "\n",
    "# classification params\n",
    "classification_args = {\n",
    "    \"n_splits\": 10, # k-folds for cross-validation in training\n",
    "    \"random_state\": 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plotting params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plotting params\n",
    "\n",
    "with_swarm = False\n",
    "highRes = False\n",
    "if highRes:\n",
    "    img_ext = '.pdf'\n",
    "else:\n",
    "    img_ext = '.png'\n",
    "\n",
    "# for CNV data\n",
    "cnv_plot_kwargs = {\n",
    "    \"vmin\": -2,\n",
    "    \"vmax\": +2,\n",
    "    \"mincol\": \"red\",\n",
    "    \"midcol\": \"white\",\n",
    "    \"maxcol\": \"blue\",\n",
    "    \"function_dict\": None\n",
    "}\n",
    "\n",
    "# for mutation data\n",
    "var_plot_kwargs = {\n",
    "    \"vmin\": 0,\n",
    "    \"vmax\": 4,\n",
    "    \"mincol\": \"white\",\n",
    "    \"midcol\": \"orange\",\n",
    "    \"maxcol\": \"red\",\n",
    "    \"function_dict\": {\n",
    "        \"no mutation\": 0,\n",
    "        \"missense\": 1,\n",
    "        \"nonframeshiftIndel\": 2,\n",
    "        \"nonsense\": 3,\n",
    "        \"frameshiftIndel\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "# for mixed features from CNV and mutation data\n",
    "mixed_plot_kwargs = {\n",
    "    \"vmin\": -4,\n",
    "    \"vmax\": +4,\n",
    "    \"mincol\": \"red\",\n",
    "    \"midcol\": \"white\",\n",
    "    \"maxcol\": \"purple\",\n",
    "    \"function_dict\": None\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "file paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data files\n",
    "cnv_data_fpath = \"output/headneck/integrate_cohorts/c1c2/CNV_mapped_filt_90/integrated_data.csv\"\n",
    "var_data_fpath = \"output/headneck/integrate_cohorts/c1c2/genepanel/integrated_data.csv\"\n",
    "\n",
    "\n",
    "# sample_info file\n",
    "sample_info_fpath = \"output/headneck/integrate_cohorts/c1c2/integrated_sample_info.csv\"\n",
    "sample_class_column = \"Relapsed\"\n",
    "sample_class_labels = [\"relapsed\",\"NOTrelapsed\"]\n",
    "sample_class_values = [1,0]\n",
    "\n",
    "sample_dataset_column = \"dataset\"\n",
    "sample_dataset_labels = [\"c1\",\"c2\"]\n",
    "sample_dataset_values = [0,1]\n",
    "\n",
    "# arguments to load the sample_info file\n",
    "sample_info_read_csv_kwargs = {\n",
    "    \"sep\": \"\\t\",\n",
    "    \"header\": 0,\n",
    "    \"col_as_index\":\"patientID\"\n",
    "}\n",
    "\n",
    "\n",
    "# genes_info files\n",
    "# ideally we would have one file for all genes regardless of the dataset they come from\n",
    "# but in our case we had to gather this information during contruction of the data\n",
    "# that is why we should take into account all files from all the data we integrate\n",
    "#  -- in this case we integrate: oncoscan filtered by Nexus, and ExCavator2 filtered by us (0.90 ProbCall) \n",
    "genes_info_names = ['c1_oncoscan_byNexus', 'c2_excavator2_byNexus_filt_90']\n",
    "genes_info_fpaths = []\n",
    "for name in genes_info_names:\n",
    "    genes_info_fpaths.append(\"output/headneck/setup_\"+name+\"/genes_info.csv\")\n",
    "\n",
    "# column names in those files we will need later in the analysis\n",
    "chr_col = 'chr_int'\n",
    "gene_id_col = 'gene'\n",
    "\n",
    "\n",
    "# output dir\n",
    "output_directory = \"output/headneck/classification/notebook/\"+reportName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select features: fpaths where the candidate features were saved from a previous processing step\n",
    "# (that pipeline is defined in the gene_signatures.feature_selection module)\n",
    "genepanel_fpath = \"output/headneck/setup_c1_genepanel/process_select_primary/data_processed.csv\"\n",
    "genepanel_key_name = 'genepanel'\n",
    "\n",
    "feature_dirs = ['c1_prmr_OncFltNxEx', 'c2_ExcvFltNxEx', 'c1_prmr_mapped_c2_CnvNxEx', 'c1_prmr_mapped_c2_Cnv', 'c1_prmr_mapped_c2_CnvMixedNxEx']\n",
    "feature_key_names = ['c1_OncFltNxEx', 'c2_ExcvFltNxEx', 'c3_CnvNxEx', 'c3_Cnv', 'c3_CnvMixedNxEx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# choose conbinations of features (optional, else set it to None)\n",
    "feature_combinations = {\n",
    "    'GnPnl_c1_OncFltNxEx': ['genepanel', 'c1_OncFltNxEx'], # 26 union\n",
    "    'GnPnl_c2_ExcvFltNxEx': ['genepanel', 'c2_ExcvFltNxEx'],  # 45 union\n",
    "    'GnPnl_c3_CnvNxEx': ['genepanel', 'c3_CnvNxEx'],  # 106 union\n",
    "#     'c3_CNVmix': ['c3_CnvNxEx', 'c3_Cnv', 'c3_CnvMixedNxEx'], # 1 common, 1337 union\n",
    "    'CNVc1c2': ['c1_OncFltNxEx', 'c2_ExcvFltNxEx'], # 45 union, 0 common\n",
    "    'GnPnl_CNVc1c2': ['genepanel', 'c1_OncFltNxEx', 'c2_ExcvFltNxEx'], # 58 union\n",
    "    'CNVcAll': ['c1_OncFltNxEx', 'c2_ExcvFltNxEx', 'c3_CnvNxEx'], # 127 union\n",
    "    'GnPnl_CNVcAll': ['genepanel', 'c1_OncFltNxEx', 'c2_ExcvFltNxEx', 'c3_CnvNxEx'] # 140 union\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SET ENVIRONMENT"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom imports\n",
    "from omics_processing.io import (\n",
    "    set_directory, load_clinical\n",
    ")\n",
    "from omics_processing.remove_duplicates import (\n",
    "    remove_andSave_duplicates\n",
    ")\n",
    "from gene_signatures.core import (\n",
    "    custom_div_cmap,\n",
    "    get_chr_ticks,\n",
    "    choose_samples,\n",
    "    parse_arg_type,\n",
    "    boxplot,\n",
    "    set_heatmap_size,\n",
    "    set_cbar_ticks,\n",
    "    edit_names_with_duplicates,\n",
    "    plot_confusion_matrix,\n",
    "    define_plot_args,\n",
    "    plot_scatter_scores,\n",
    "    plot_roc_with_std_for_one_model,\n",
    "    plot_roc_for_many_models,\n",
    "    compute_and_plot_confusion_matrices,\n",
    "    plot_prediction_counts_per_class,\n",
    "    plot_data_heatmap,\n",
    "    extract_gene_set,\n",
    "    save_image,\n",
    "    check_path_integrity,\n",
    "    plot_df_hist,\n",
    "    balanced_train_test_split\n",
    ")\n",
    "\n",
    "# basic imports\n",
    "import os, sys\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import json\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from natsort import natsorted, index_natsorted\n",
    "import math\n",
    "import logging\n",
    "from sklearn import linear_model\n",
    "from sklearn import svm\n",
    "from distutils.util import strtobool\n",
    "from scipy.stats import binom_test\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.model_selection import StratifiedKFold, train_test_split\n",
    "from sklearn.calibration import CalibratedClassifierCV\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "from scipy import interp\n",
    "from functools import reduce\n",
    "\n",
    "# plotting imports\n",
    "%matplotlib inline\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set_style('white')\n",
    "sns.set_context('talk')\n",
    "\n",
    "script_path = os.getcwd()\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _run_classification(\n",
    "        dat, dat_target, random_state=None, n_splits=10):\n",
    "\n",
    "    min_class_count = np.unique(dat_target, return_counts=True)[1].min()\n",
    "    if n_splits is not None:\n",
    "        if (n_splits > dat.shape[0]) or (n_splits > min_class_count):\n",
    "            n_splits = min_class_count\n",
    "    if random_state is not None:\n",
    "        random_state = parse_arg_type(random_state, int)\n",
    "    else:\n",
    "        random_state = 0\n",
    "    logger.info(\n",
    "        \"model: svm.LinearSVC with l2 penalty, squared_hinge loss \" +\n",
    "        \"and random_state: \"+str(random_state)\n",
    "    )\n",
    "    model = svm.LinearSVC(\n",
    "        penalty='l2', C=1, random_state=random_state,\n",
    "        loss='squared_hinge', dual=False\n",
    "    )\n",
    "\n",
    "    logger.info(\"Running classification...\")\n",
    "    dat = dat.copy()\n",
    "    dat_target = dat_target.copy()\n",
    "\n",
    "    X = dat\n",
    "    y = dat_target\n",
    "    k_fold = StratifiedKFold(n_splits=n_splits)\n",
    "    cross_val_scores = []\n",
    "    all_coefs = np.zeros((n_splits, dat.shape[1]))\n",
    "    y_train_predictions = pd.Series(index=y.index)\n",
    "    y_train_predictions.name = \"train_predictions\"\n",
    "    \n",
    "    fprs = []\n",
    "    tprs = []\n",
    "    interps = []\n",
    "    aucs = []\n",
    "    mean_fpr = np.linspace(0, 1, 100)\n",
    "\n",
    "    split_i = 0\n",
    "    for train_indices, test_indices in k_fold.split(X, y):\n",
    "        X_train = dat.iloc[train_indices]\n",
    "        y_train = dat_target.iloc[train_indices]\n",
    "        \n",
    "        X_crossval = dat.iloc[test_indices]\n",
    "        y_crossval = dat_target.iloc[test_indices]\n",
    "\n",
    "        model.fit(X_train, y_train)\n",
    "        all_coefs[split_i:split_i+1, :] = model.coef_[0]\n",
    "        cross_val_scores.append(model.score(X_crossval, y_crossval))\n",
    "        y_train_predictions.iloc[test_indices] = model.predict(X_crossval)\n",
    "        \n",
    "        \n",
    "        y_proba = model.decision_function(X_crossval)\n",
    "        # clf = CalibratedClassifierCV(base_estimator=model, cv='prefit')\n",
    "        # clf.fit(X_crossval, y_crossval)\n",
    "        # y_proba = clf.predict_proba(X_crossval)\n",
    "        # Compute ROC curve and area the curve\n",
    "        # fpr, tpr, thresholds = roc_curve(y_crossval, y_proba[:, 1])\n",
    "        fpr, tpr, thresholds = roc_curve(y_crossval, y_proba)\n",
    "        fprs.append(fpr)\n",
    "        tprs.append(tpr)\n",
    "        interps.append(interp(mean_fpr, fpr, tpr))\n",
    "        interps[-1][0] = 0.0\n",
    "        roc_auc = auc(fpr, tpr)\n",
    "        aucs.append(roc_auc)\n",
    "\n",
    "        split_i += 1\n",
    "\n",
    "    X = dat\n",
    "    y = dat_target\n",
    "    model.fit(X, y)\n",
    "\n",
    "    all_coefs = pd.DataFrame(all_coefs, columns=dat.columns.values)\n",
    "\n",
    "    return model, all_coefs, y_train_predictions, cross_val_scores, fprs, tprs, interps, aucs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# START ANALYSIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### check_path_integrity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# properly set file paths\n",
    "try:\n",
    "    os.path.exists(MainDataDir)\n",
    "except:\n",
    "    MainDataDir = os.path.join(script_path, '..','..', 'data')\n",
    "    logger.debug(\"set MainDataDir:\\n\"+MainDataDir)\n",
    "\n",
    "# data output\n",
    "output_directory = check_path_integrity(output_directory, rootDir=MainDataDir, name=\"output\", force=True)\n",
    "\n",
    "if DEBUG:\n",
    "    logging.basicConfig(stream=sys.stdout, level=logging.DEBUG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cnv data input\n",
    "cnv_data_fpath = check_path_integrity(cnv_data_fpath, rootDir=MainDataDir, name=\"cnv data\")\n",
    "\n",
    "# genepanel data input\n",
    "var_data_fpath = check_path_integrity(var_data_fpath, rootDir=MainDataDir, name=\"var data\")\n",
    "\n",
    "# sample info input\n",
    "sample_info_fpath = check_path_integrity(sample_info_fpath, rootDir=MainDataDir, name=\"sample_info\")\n",
    "\n",
    "# gene info input\n",
    "for i,fpath in enumerate(genes_info_fpaths):\n",
    "    genes_info_fpaths[i] = check_path_integrity(fpath, rootDir=MainDataDir, name=\"gene_info\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fpaths_dict\n",
    "fpaths_dict = {}\n",
    "fpaths_dict[genepanel_key_name] = check_path_integrity(genepanel_fpath, rootDir=MainDataDir, name=\"genepanel features\")\n",
    "\n",
    "for _f, _k in zip(feature_dirs, feature_key_names):\n",
    "    fpath = \"output/headneck/feature_selection/\"+_f+\"/featsel_results.csv\"\n",
    "    fpaths_dict[_k] = check_path_integrity(fpath, rootDir=MainDataDir, name=_k+\" features\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cnv data\n",
    "cnv_data = pd.read_csv(cnv_data_fpath, sep='\\t', header=0, index_col=0)\n",
    "logger.info('loaded cnv data file with shape: '+str(cnv_data.shape))\n",
    "\n",
    "cnv_data.columns += \"__CNV\"\n",
    "\n",
    "cnv_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load genepanel data\n",
    "var_data = pd.read_csv(var_data_fpath, sep='\\t', header=0, index_col=0)\n",
    "logger.info('loaded var data file with shape: '+str(var_data.shape))\n",
    "\n",
    "var_data.columns += \"__VAR\"\n",
    "\n",
    "var_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load info table of samples\n",
    "sample_info = load_clinical(\n",
    "    sample_info_fpath, **sample_info_read_csv_kwargs)\n",
    "logger.info('loaded sample_info file with shape: '+str(sample_info.shape))\n",
    "\n",
    "sample_info = sample_info.loc[cnv_data.index,:]\n",
    "logger.info('keeping part of sample_infowith shape: '+str(sample_info.shape))\n",
    "\n",
    "sample_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load info table of genes\n",
    "dfs = []\n",
    "for i, fpath in enumerate(genes_info_fpaths):\n",
    "    df = pd.read_csv(fpath, sep='\\t', header=0, index_col=0)\n",
    "    df.columns.name = genes_info_names[i]\n",
    "    dfs.append(df)\n",
    "    logger.info('loaded a genes info file with shape: '+str(df.shape)+'\\nfrom: '+fpath)\n",
    "\n",
    "genes_info = reduce(\n",
    "    lambda left,right: pd.merge(left, right, on=gene_id_col, how='inner', suffixes=['','__'+str(right.columns.name)]), dfs)\n",
    "logger.info('FINAL genes info file with shape: '+str(genes_info.shape))\n",
    "\n",
    "genes_info[gene_id_col] += \"__CNV\"\n",
    "\n",
    "genes_info.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the ground truth\n",
    "ground_truth = sample_info.loc[cnv_data.index, sample_class_column]\n",
    "\n",
    "ground_truth.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data heatmaps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for plots\n",
    "sample_class_labels = np.array(sample_class_labels)\n",
    "sample_class_values = np.array(sample_class_values)\n",
    "\n",
    "adict = define_plot_args(**cnv_plot_kwargs)\n",
    "# update cnv_plot_kwargs with adict\n",
    "cnv_plot_kwargs.update(adict)\n",
    "\n",
    "adict = define_plot_args(**var_plot_kwargs)\n",
    "# update cnv_plot_kwargs with adict\n",
    "var_plot_kwargs.update(adict)\n",
    "\n",
    "adict = define_plot_args(**mixed_plot_kwargs)\n",
    "# update cnv_plot_kwargs with adict\n",
    "mixed_plot_kwargs.update(adict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot Heatmap of genepanel_data\n",
    "plot_data_heatmap(\n",
    "    var_data, ground_truth, **var_plot_kwargs\n",
    ")\n",
    "plt.title('var data: '+str(var_data.shape[1])+' gene profiles')\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"heatmap_var_data\", img_ext=img_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot Heatmap of cnv_data w/ duplicates\n",
    "xlabels, xpos = get_chr_ticks(\n",
    "    genes_info, cnv_data, id_col=gene_id_col, chr_col=chr_col)\n",
    "\n",
    "plot_data_heatmap(\n",
    "    cnv_data, ground_truth, xlabels, xpos, **cnv_plot_kwargs\n",
    ")\n",
    "plt.title('cnv data: '+str(cnv_data.shape[1])+' gene profiles')\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"heatmap_cnv_data\", img_ext=img_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Find and Remove duplicate gene profiles in the (integrated) data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove all zero columns!\n",
    "orphancols = np.where(abs(cnv_data).sum(axis=0) == 0)[0]\n",
    "if len(orphancols) > 0:\n",
    "    logger.warning('removing '+str(len(orphancols))+' genes from cnv data with zero columns!')\n",
    "    cols2drop = cnv_data.columns.values[orphancols]\n",
    "    cnv_data = cnv_data.drop(cols2drop, axis=1).copy()\n",
    "\n",
    "# REMOVE DUPLICATES!!!!\n",
    "cnv_data_uniq, dupldict, wo_dupl_set, all_dupl_set = remove_andSave_duplicates(\n",
    "    cnv_data, to_compute_euclidean_distances=to_compute_euclidean_distances,\n",
    "    to_save_euclidean_distances=to_save_euclidean_distances, to_save_output=True,\n",
    "    output_filename='cnv_data_wo_duplicates',\n",
    "    output_directory=output_directory\n",
    ")\n",
    "single_dupl_set = set(dupldict.keys())\n",
    "\n",
    "_countA = len(set.union(single_dupl_set, wo_dupl_set))\n",
    "_countB = cnv_data_uniq.shape[1]\n",
    "if not _countA == _countB:\n",
    "    print(\n",
    "        'ERROR: inconsistencies in the final uniq gene count!\\n'+\n",
    "        str(_countA)+' genes that should be in the uniq dataset VS. '+\n",
    "        str(_countB)+' genes that are'\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merge the CNV (w/o duplicates) and VAR features in the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# combine var and cnv features\n",
    "data_uniq = pd.concat([cnv_data_uniq, var_data], axis=1, join='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot cnv data heatmaps again, now w/o duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot Heatmap of cnv data w/o duplicates\n",
    "xlabels_uniq, xpos_uniq = get_chr_ticks(\n",
    "    genes_info, cnv_data_uniq, id_col=gene_id_col, chr_col=chr_col)\n",
    "\n",
    "plot_data_heatmap(\n",
    "    cnv_data_uniq, ground_truth, xlabels_uniq, xpos_uniq, **cnv_plot_kwargs\n",
    ")\n",
    "plt.title('cnv data w/o duplicates: '+str(cnv_data_uniq.shape[1])+' gene profiles')\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"heatmap_cnv_data_uniq\", img_ext=img_ext)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#  Plot Heatmap of mixed data w/o duplicates\n",
    "plot_data_heatmap(\n",
    "    data_uniq, ground_truth, None, None, **mixed_plot_kwargs\n",
    ")\n",
    "plt.title('mixed data w/o duplicates: '+str(data_uniq.shape[1])+' gene profiles')\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"heatmap_mixed_data_uniq\", img_ext=img_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split data in train-test sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import ShuffleSplit\n",
    "split_with_stratify = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels_to_check = pd.concat([ground_truth, sample_info[sample_dataset_column]], axis=1, sort=False)\n",
    "if split_with_stratify:\n",
    "    # split data in train-test ONCE!\n",
    "    stratify_by = labels_to_check.copy()\n",
    "    stratify_by = stratify_by.loc[ground_truth.index]\n",
    "\n",
    "    data_train, data_test, y_train, y_test = train_test_split(\n",
    "        data_uniq, ground_truth,\n",
    "        train_size=split_train_size,\n",
    "        test_size=None,\n",
    "        random_state=split_random_state,\n",
    "        stratify=stratify_by\n",
    "    )\n",
    "else:\n",
    "    data_train, data_test, y_train, y_test = balanced_train_test_split(\n",
    "        data_uniq, ground_truth,\n",
    "        train_size=split_train_size, \n",
    "        random_state=split_random_state)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnv_data_train = cnv_data.loc[data_train.index,:].copy()\n",
    "cnv_data_test = cnv_data.loc[data_test.index,:].copy()\n",
    "\n",
    "var_data_train = var_data.loc[data_train.index,:].copy()\n",
    "var_data_test = var_data.loc[data_test.index,:].copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### plot the sample counts per class and per data source for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "annot_dict = {\n",
    "    sample_class_column:{\n",
    "        'labels': sample_class_labels,\n",
    "        'values': sample_class_values\n",
    "    },\n",
    "    sample_dataset_column:{\n",
    "        'labels': sample_dataset_labels,\n",
    "        'values': sample_dataset_values\n",
    "    }\n",
    "}\n",
    "\n",
    "# plot histograms of stratification columns for each data split set\n",
    "plot_df_hist(\n",
    "    labels_to_check,\n",
    "    annot_dict,\n",
    "    by=None, column=None, figsize=(8,4))\n",
    "plt.suptitle('all '+str(ground_truth.shape[0])+' samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_all_samples\", img_ext=img_ext)\n",
    "\n",
    "plot_df_hist(\n",
    "    labels_to_check.loc[cnv_data_train.index],\n",
    "    annot_dict,\n",
    "    by=None, column=None, figsize=(8,4))\n",
    "plt.suptitle(str(y_train.shape[0])+' train samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_train_samples\", img_ext=img_ext)\n",
    "\n",
    "plot_df_hist(\n",
    "    labels_to_check.loc[cnv_data_test.index],\n",
    "    annot_dict,\n",
    "    by=None, column=None, figsize=(8,4))\n",
    "plt.suptitle(str(y_test.shape[0])+' test samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_test_samples\", img_ext=img_ext)\n",
    "\n",
    "\n",
    "# plot histograms of stratification columns for each data split set, per data source\n",
    "plot_df_hist(\n",
    "    labels_to_check,\n",
    "    annot_dict,\n",
    "    by=sample_class_column, column=sample_dataset_column, figsize=(8,4))\n",
    "plt.suptitle('all '+str(ground_truth.shape[0])+' samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_all_samples_per\"+sample_dataset_column, img_ext=img_ext)\n",
    "\n",
    "plot_df_hist(\n",
    "    labels_to_check.loc[cnv_data_train.index],\n",
    "    annot_dict,\n",
    "    by=sample_class_column, column=sample_dataset_column, figsize=(8,4))\n",
    "plt.suptitle(str(y_train.shape[0])+' train samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_train_samples_per\"+sample_dataset_column, img_ext=img_ext)\n",
    "\n",
    "plot_df_hist(\n",
    "    labels_to_check.loc[cnv_data_test.index],\n",
    "    annot_dict,\n",
    "    by=sample_class_column, column=sample_dataset_column, figsize=(8,4))\n",
    "plt.suptitle(str(y_test.shape[0])+' test samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_test_samples_per\"+sample_dataset_column, img_ext=img_ext)\n",
    "\n",
    "\n",
    "# plot histograms of stratification columns for each data split set, per sample class\n",
    "plot_df_hist(\n",
    "    labels_to_check,\n",
    "    annot_dict,\n",
    "    by=sample_dataset_column, column=sample_class_column, figsize=(8,4))\n",
    "plt.suptitle('all '+str(ground_truth.shape[0])+' samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_all_samples_per\"+sample_class_column, img_ext=img_ext)\n",
    "\n",
    "plot_df_hist(\n",
    "    labels_to_check.loc[cnv_data_train.index],\n",
    "    annot_dict,\n",
    "    by=sample_dataset_column, column=sample_class_column, figsize=(8,4))\n",
    "plt.suptitle(str(y_train.shape[0])+' train samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_train_samples_per\"+sample_class_column, img_ext=img_ext)\n",
    "\n",
    "plot_df_hist(\n",
    "    labels_to_check.loc[cnv_data_test.index],\n",
    "    annot_dict,\n",
    "    by=sample_dataset_column, column=sample_class_column, figsize=(8,4))\n",
    "plt.suptitle(str(y_test.shape[0])+' test samples', fontsize=16)\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"labels_to_check_test_samples_per\"+sample_class_column, img_ext=img_ext)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot data heatmaps for each set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CNV\n",
    "\n",
    "xlabels_train, xpos_train = get_chr_ticks(\n",
    "    genes_info, cnv_data_train, id_col=gene_id_col, chr_col=chr_col)\n",
    "\n",
    "xlabels_test, xpos_test = get_chr_ticks(\n",
    "    genes_info, cnv_data_test, id_col=gene_id_col, chr_col=chr_col)\n",
    "\n",
    "#  Plot Heatmap of train cnv data (w/o duplicates)\n",
    "plot_data_heatmap(\n",
    "    cnv_data_train, y_train, xlabels_train, xpos_train, **cnv_plot_kwargs\n",
    ")\n",
    "plt.title('cnv train data: '+str(cnv_data_train.shape[1])+' gene profiles')\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"heatmap_cnv_data_train\", img_ext=img_ext)\n",
    "\n",
    "#  Plot Heatmap of test cnv data (w/o duplicates)\n",
    "plot_data_heatmap(\n",
    "    cnv_data_test, y_test, xlabels_test, xpos_test, **cnv_plot_kwargs\n",
    ")\n",
    "plt.title('cnv test data: '+str(cnv_data_test.shape[1])+' gene profiles')\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"heatmap_cnv_data_test\", img_ext=img_ext)\n",
    "\n",
    "\n",
    "# VAR\n",
    "\n",
    "#  Plot Heatmap of train var data\n",
    "plot_data_heatmap(\n",
    "    var_data_train, y_train, None, None, **var_plot_kwargs\n",
    ")\n",
    "plt.title('var train data: '+str(var_data_train.shape[1])+' genes')\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"heatmap_var_data_train\", img_ext=img_ext)\n",
    "\n",
    "#  Plot Heatmap of test var data\n",
    "plot_data_heatmap(\n",
    "    var_data_test, y_test, None, None, **var_plot_kwargs\n",
    ")\n",
    "plt.title('var test data: '+str(var_data_test.shape[1])+' genes')\n",
    "save_image(saveReport=saveReport, output_directory=output_directory, img_name=\"heatmap_var_data_test\", img_ext=img_ext)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set operations for each features set we consider for classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### initialize the sets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "set of all features in the integrated dataset, w/o and w/o (cnv) duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_cnv_data_genes = set(cnv_data.columns.values)\n",
    "all_cnv_data_genes_uniq = set(cnv_data_uniq.columns.values)\n",
    "\n",
    "all_var_data_genes = set(var_data.columns.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sets of features per source, each source will be a seperate model in the classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_dict = {}\n",
    "features_sets = {}\n",
    "for key in fpaths_dict:\n",
    "    n_total_pre_filt = n_total_pre_top = None\n",
    "    df = pd.read_csv(fpaths_dict[key], sep='\\t', header=0, index_col=0)\n",
    "    \n",
    "    if genepanel_key_name in key:\n",
    "        df.columns += \"__VAR\"\n",
    "        features_sets[key] = set(df.columns.values)\n",
    "        n_final = len(features_sets[key])\n",
    "        n_unique = n_final\n",
    "    else:\n",
    "        features_dict[key] = df\n",
    "        if abs_mean_filter is not None:\n",
    "            temp = extract_gene_set(df)\n",
    "            n_total_pre_filt = n_total = len(temp)\n",
    "            df = df[df['abs_mean_coef'] > abs_mean_filter].copy()\n",
    "        elif topN is not None:\n",
    "            temp = extract_gene_set(df)\n",
    "            n_total_pre_top = n_total = len(temp)\n",
    "            df = df.sort_values(by='abs_mean_coef', ascending=False).iloc[:topN,:].copy()\n",
    "        _gene_set = extract_gene_set(df)\n",
    "        _gene_set = set([s + \"__CNV\" for s in _gene_set])      \n",
    "        features_sets[key] = _gene_set\n",
    "        n_final = len(features_sets[key])\n",
    "        n_unique = df.shape[0]\n",
    "        \n",
    "    if n_total_pre_filt is not None:\n",
    "        logger.info(\n",
    "            str(n_unique)+' unique out of '+str(n_final)+' filtered out of '\n",
    "            +str(n_total)+' total features from '+key)\n",
    "    elif n_total_pre_top is not None:\n",
    "        logger.info(\n",
    "            'top'+str(topN)+': '+str(n_unique)+' unique out of '+str(n_final)+' filtered out of '\n",
    "            +str(n_total)+' total features from '+key)\n",
    "    else:\n",
    "        logger.info(str(n_unique)+' unique out of '+str(n_final)+' total features from '+key)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Venn diagrams to explain the functionality of the process below:<br>\n",
    "U --> data uniq genes (genes w/o dupl + single copy duplicates genes) <br>\n",
    "D --> the rest of the duplicates genes copies<br>\n",
    "fs --> a single features set<br>\n",
    "IU --> the features that exist in the U set<br>\n",
    "ID --> the features that exist in the D set<br>\n",
    "NI --> the features that do NOT exist in neither set<br>\n",
    "IDa --> the features that exist in the ID set and are not represented in the IU set<br>\n",
    "IDb --> the features that exist in the ID set and are already represented in the IU set<br>\n",
    "_ID = IDa + IDb_<br>\n",
    "U_IDa --> the features that exist in the U set (but not in the IU set) and represent the IDa features<br>\n",
    "**fs in data_uniq = IU + U_IDa**<br>\n",
    "<img src=\"./files/venn_legend.jpg?1\" alt=\"drawing\" style=\"float:left\" width=\"300px\"/>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# U: all_data_genes_uniq\n",
    "# D: all_dupl_set.difference(all_data_genes_uniq)\n",
    "# fs: features_sets[key]\n",
    "new_features_sets = {}\n",
    "\n",
    "for key in features_sets:\n",
    "    if genepanel_key_name in key:\n",
    "        U_set = all_var_data_genes\n",
    "        D_set = set()\n",
    "    else:\n",
    "        U_set = all_cnv_data_genes_uniq\n",
    "        D_set = all_dupl_set.difference(all_cnv_data_genes_uniq)\n",
    "            \n",
    "    fs = features_sets[key]\n",
    "    _fs_original_size = len(fs)\n",
    "    print(key+' feature set :')\n",
    "    print('--- originally ---')\n",
    "    print(' original total size: '+str(_fs_original_size))\n",
    "    IU_set = fs.intersection(U_set)\n",
    "    _IU_size = len(IU_set)\n",
    "    print(' IU_set: '+str(_IU_size))\n",
    "\n",
    "    ID_set = (fs.difference(IU_set)).intersection(D_set)\n",
    "    _ID_size = len(ID_set)\n",
    "    print(' ID_set: '+str(_ID_size))\n",
    "\n",
    "    NI_set = (fs.difference(IU_set)).difference(ID_set)\n",
    "    _NI_size =len(NI_set)\n",
    "    print(' NI_set: '+str(_NI_size))\n",
    "\n",
    "    U_IDa_set = set()\n",
    "    IDa_set = set()\n",
    "    IDb_set = set()\n",
    "    IDc_set = set()\n",
    "    done = False\n",
    "    temp_set = ID_set.copy()\n",
    "    while temp_set and not done:\n",
    "        for ud, dl in dupldict.items():\n",
    "            _IDx = set(dl).intersection(temp_set)\n",
    "            if _IDx:\n",
    "                temp_set = temp_set.difference(_IDx)\n",
    "                if ud not in IU_set:\n",
    "                    U_IDa_set.add(ud)\n",
    "                    IDa_set.update(_IDx)\n",
    "                else:\n",
    "                    IDb_set.update(_IDx) \n",
    "        done = True\n",
    "\n",
    "    fs_in_data_uniq = set.union(IU_set, U_IDa_set)\n",
    "    _fs_in_data_uniq_size = len(fs_in_data_uniq)\n",
    "\n",
    "    ########################################\n",
    "    new_features_sets[key] = fs_in_data_uniq\n",
    "    ########################################\n",
    "    \n",
    "    print('--- finally ---')\n",
    "    print(' features in data uniq : '+str(_fs_in_data_uniq_size))\n",
    "    print(' IU_set: '+str(_IU_size))\n",
    "    print('   U_IDa_set: '+str(len(U_IDa_set)))\n",
    "    print(' ID_set: '+str(len(ID_set)))\n",
    "    print('   IDa_set: '+str(len(IDa_set)))\n",
    "    print('   IDb_set: '+str(len(IDb_set)))\n",
    "    print(' NI_set: '+str(_NI_size))\n",
    "\n",
    "    if ID_set != set.union(IDa_set, IDb_set):\n",
    "        print(\n",
    "            'ERROR: something went wrong, ID_set != IDa_set + IDb_set, for feature set: '+key\n",
    "        )\n",
    "        break\n",
    "\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Venn diagrams to explain the feature set in the current data in respect to how it was in the original data source it was selected from<br>\n",
    "Process:<br>\n",
    "- for every feature set \n",
    "- take the intersection with the data (w/o duplicates), \n",
    "- save a table with the intersection's features info about position and their duplicates\n",
    "- save another table with the remaining features info <br>\n",
    "<img src=\"./files/venn_legend_2.jpg?1\" alt=\"drawing\" style=\"float:left\" width=\"500px\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "update the dictionaries of the feature sets to include the combinations too (if there are any)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if feature_combinations is not None:\n",
    "    new_features_sets.update({\n",
    "        key: set.union(\n",
    "            *[new_features_sets[x] for x in feature_combinations[key]]\n",
    "        ) for key in feature_combinations\n",
    "    })\n",
    "\n",
    "    features_sets.update({\n",
    "        key: set.union(\n",
    "            *[features_sets[x] for x in feature_combinations[key]]\n",
    "        ) for key in feature_combinations\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# for every feature set in the new_features_sets (even the combinations)\n",
    "# that is corresponding to data_uniq, \n",
    "# save a table with the feature names as index\n",
    "# and their duplicates, from dupldict in another column\n",
    "# include also other columns, like chr, start and end from the gene_info table\n",
    "# save also the positions of these duplicate genes\n",
    "# finally save separately the info on the remaining features, the ones not in the data\n",
    "\n",
    "for key in new_features_sets.keys():\n",
    "    print(key)\n",
    "    fset = list(new_features_sets[key])\n",
    "    # a table with the feature names as index\n",
    "    feature_table = pd.DataFrame(index=fset)\n",
    "    # the duplicates, from dupldict\n",
    "    feature_table['dupl_features'] = \\\n",
    "        feature_table.index.map(dupldict).values\n",
    "    # include other columns, like chr, start and end from the gene_info table\n",
    "    feature_table = pd.concat([feature_table, genes_info.set_index(gene_id_col)], axis=1, join='inner')\n",
    "\n",
    "    # save the chr and positions of the duplicate genes\n",
    "    for agene in feature_table.index:\n",
    "        if agene in dupldict.keys():\n",
    "            l = [agene]\n",
    "            l.extend(dupldict[agene])\n",
    "            feature_table.loc[agene, 'aggChrGene'] = str(natsorted(\n",
    "                        genes_info.set_index(\n",
    "                            gene_id_col).loc[l].reset_index().groupby(\n",
    "                                by=['chr'])['gene'].apply(\n",
    "                                    lambda x: list(np.unique(np.append([], x)))\n",
    "                                    ).reset_index().values.tolist()))\n",
    "            aggPos = \\\n",
    "                genes_info.set_index(gene_id_col).loc[l].groupby(\n",
    "                    by=['chr']).agg(\n",
    "                        {'start': min, 'end': max}\n",
    "                        ).reset_index().astype(str).apply(\n",
    "                            lambda x: ':'.join(x), axis=1).values\n",
    "            feature_table.loc[agene, 'aggPos'] = np.apply_along_axis(\n",
    "                        lambda x: '__'.join(x), 0, natsorted(aggPos))\n",
    "\n",
    "    # get separately the info on the remaining features, the ones not in the data\n",
    "    remaining_features = pd.DataFrame(index=sorted(list(features_sets[key].difference(new_features_sets[key]))))\n",
    "    # info on chr, start, end etc. from the gene_info table\n",
    "    remaining_features = remaining_features.join(genes_info.set_index(gene_id_col), how='left')\n",
    "    index_name = remaining_features.index.name = 'gene'\n",
    "    # remove any __XX..X suffix to be able to cross-reference with the original features_dict\n",
    "    remaining_features['cleanName'] = \\\n",
    "                remaining_features.reset_index()[index_name]\\\n",
    "                .str.split('__', expand=True)[0].values\n",
    "\n",
    "    if key in features_dict:\n",
    "        # cross-reference with the original features_dict\n",
    "        df = features_dict[key].copy()\n",
    "        if 'cleanName' not in df.columns:\n",
    "            index_name = df.index.name = 'dummy'\n",
    "            df['cleanName'] = \\\n",
    "                df.reset_index()[index_name]\\\n",
    "                .str.split('__', expand=True)[0].values\n",
    "            \n",
    "        for row in df.index:\n",
    "            # for each row add the remaining features that match it\n",
    "            if df.loc[row,'dupl_genes'] is not np.nan:\n",
    "                original_dupl_gene_set = set([df.loc[row,'cleanName']]).union(eval(df.loc[row,'dupl_genes']))\n",
    "                remaining_gene_set = set(remaining_features['cleanName'].values)\n",
    "                row_dupl_gene_set = remaining_gene_set.intersection(original_dupl_gene_set)\n",
    "                df.loc[row,'remaining_dupl_genes'] = str(list(row_dupl_gene_set))\n",
    "\n",
    "        # if there are remaining features that do not match any row \n",
    "        #(although this should probably never happen)\n",
    "        # then add them as new rows \n",
    "        # and add info on chr, start, end etc. from the gene_info table\n",
    "        original_dupl_gene_set = extract_gene_set(features_dict[key])\n",
    "        new_dupl_gene_set = extract_gene_set(df)\n",
    "        remaining_uniq_gene_set = original_dupl_gene_set.difference(new_dupl_gene_set)\n",
    "        if remaining_uniq_gene_set:\n",
    "            df = pd.concat(\n",
    "                [df, remaining_features.loc[list(remaining_uniq_gene_set),:]], \n",
    "                axis=0, join='outer', sort=False\n",
    "            )\n",
    "\n",
    "        remaining_features_with_info = df.copy()\n",
    "    else:\n",
    "        remaining_features_with_info = remaining_features\n",
    "\n",
    "    # save as tab-delimited csv file\n",
    "    fname = 'features_'+key+'_in_data.csv'\n",
    "    fpath = os.path.join(output_directory, fname)\n",
    "    logger.info(\"-save combined features in :\\n\"+fpath)\n",
    "    feature_table.to_csv(\n",
    "        fpath, sep='\\t', header=True, index=True)\n",
    "    \n",
    "    fname = 'features_'+key+'_NOT_in_data.csv'\n",
    "    fpath = os.path.join(output_directory, fname)\n",
    "    logger.info(\"-save combined features in :\\n\"+fpath)\n",
    "    remaining_features_with_info.to_csv(\n",
    "        fpath, sep='\\t', header=True, index=True)\n",
    "\n",
    "    # save also as excel file\n",
    "    fname = 'features_'+key+'.xlsx'\n",
    "    fpath = os.path.join(output_directory, fname)\n",
    "    logger.info('-save csv file as excel too')\n",
    "    writer = pd.ExcelWriter(fpath)\n",
    "    feature_table.to_excel(writer, sheet_name='featuresInData')\n",
    "    remaining_features_with_info.to_excel(writer, sheet_name='featuresNOTInData')\n",
    "    writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification for every feature set (=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "fs_fprs = {}\n",
    "fs_tprs = {}\n",
    "fs_aucs = {}\n",
    "for key in new_features_sets:\n",
    "    mixed_plot_kwargs['x_ut'] = 100\n",
    "    \n",
    "    fset = natsorted(list(new_features_sets[key]))\n",
    "    X_train = data_train.loc[:,fset].copy()\n",
    "    X_test = data_test.loc[:,fset].copy()\n",
    "    \n",
    "    #  Plot Heatmap of features data w/o duplicates\n",
    "    plot_data_heatmap(\n",
    "        X_train, y_train, None, None, **mixed_plot_kwargs\n",
    "    )\n",
    "    plt.title(key+' features in train data w/o duplicates: '+str(len(fset))+' features')\n",
    "    save_image(\n",
    "        saveReport=saveReport, output_directory=output_directory, \n",
    "        img_name=\"heatmap_\"+key+\"_fs_train_data_uniq\", img_ext=img_ext)\n",
    "    \n",
    "    #  Plot Heatmap of features w/o duplicates\n",
    "    plot_data_heatmap(\n",
    "        X_test, y_test, None, None, **mixed_plot_kwargs\n",
    "    )\n",
    "    plt.title(key+' features in test data w/o duplicates: '+str(len(fset))+' features')\n",
    "    save_image(\n",
    "        saveReport=saveReport, output_directory=output_directory, \n",
    "        img_name=\"heatmap_\"+key+\"_fs_test_data_uniq\", img_ext=img_ext)\n",
    "    \n",
    "\n",
    "    # train model\n",
    "    model, all_coefs, y_train_predictions, y_train_scores, fprs, tprs, interps, aucs = \\\n",
    "        _run_classification(\n",
    "            X_train, y_train, **classification_args)\n",
    "\n",
    "    # plot_prediction_counts_per_class\n",
    "    plot_prediction_counts_per_class(\n",
    "        y_train, y_train_predictions, class_labels=sample_class_labels, class_values=sample_class_values)\n",
    "    save_image(\n",
    "        saveReport=saveReport, output_directory=output_directory, \n",
    "        img_name=\"count_predictions_per_class\", img_ext=img_ext)\n",
    "\n",
    "    # compute_and_plot_confusion_matrices\n",
    "    plt1, plt2 = compute_and_plot_confusion_matrices(\n",
    "        y_train, y_train_predictions, class_labels=sample_class_labels, class_values=sample_class_values)\n",
    "    save_image(\n",
    "        saveReport=saveReport, output_directory=output_directory, \n",
    "        img_name=\"confusion_matrix\", img_ext=img_ext, plt_obj=plt1)\n",
    "    save_image(\n",
    "        saveReport=saveReport, output_directory=output_directory, \n",
    "        img_name=\"confusion_matrix_normalized\", img_ext=img_ext, plt_obj=plt2)\n",
    "\n",
    "    # plot_roc_with_std_for_one_model\n",
    "    n_splits = classification_args[\"n_splits\"]\n",
    "    plot_roc_with_std_for_one_model(n_splits, fprs, tprs, interps, aucs, figsize=(10,10), model_name=key)\n",
    "    save_image(\n",
    "        saveReport=saveReport, output_directory=output_directory, \n",
    "        img_name=\"train_crossval_roc_curves_\"+key, img_ext=img_ext)\n",
    "\n",
    "    # Test the model\n",
    "    y_test_score = model.score(X_test, y_test)\n",
    "    y_test_predictions = model.predict(X_test)\n",
    "    y_test_predictions = pd.Series(y_test_predictions, index=X_test.index)\n",
    "    y_test_predictions.name = 'test_predictions'\n",
    "\n",
    "    # Plot scatter plot of scores\n",
    "    plot_scatter_scores(y_train_scores, y_test_score)\n",
    "    save_image(\n",
    "        saveReport=saveReport, output_directory=output_directory, \n",
    "        img_name=\"scatter_scores\", img_ext=img_ext)\n",
    "    \n",
    "    # prepare for the ROC curves on each feature set\n",
    "    y_proba = model.decision_function(X_test)\n",
    "    # Compute ROC curve and area the curve\n",
    "    fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "    fs_fprs[key] = fpr\n",
    "    fs_tprs[key] = tpr\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    fs_aucs[key] = roc_auc\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models ROC curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all models\n",
    "choose_models = list(new_features_sets.keys())\n",
    "plot_roc_for_many_models(\n",
    "    choose_models, fs_fprs, fs_tprs, fs_aucs, figsize=(10,10), \n",
    "    n_fs={key:len(new_features_sets[key]) for key in choose_models})\n",
    "save_image(\n",
    "    saveReport=saveReport, output_directory=output_directory, \n",
    "    img_name=\"test_all_models_roc_curves\", img_ext=img_ext)\n",
    "\n",
    "\n",
    "# some models: with CNVs\n",
    "choose_models = ['c1_OncFltNxEx', 'c2_ExcvFltNxEx', 'c3_CnvNxEx', 'CNVc1c2', 'CNVcAll']\n",
    "plot_roc_for_many_models(\n",
    "    choose_models, fs_fprs, fs_tprs, fs_aucs, figsize=(10,10),\n",
    "    n_fs={key:len(new_features_sets[key]) for key in choose_models}\n",
    ")\n",
    "save_image(\n",
    "    saveReport=saveReport, output_directory=output_directory, \n",
    "    img_name=\"test_some_models_roc_curves\", img_ext=img_ext)\n",
    "\n",
    "\n",
    "# 5 best models (i.r.t AUC)\n",
    "all_aucs = pd.DataFrame.from_dict(fs_aucs, orient='index', columns=['AUC'])\n",
    "choose_models = all_aucs.sort_values(by='AUC', ascending=False).iloc[:5].index.values\n",
    "plot_roc_for_many_models(\n",
    "    choose_models, fs_fprs, fs_tprs, fs_aucs, figsize=(10,10),\n",
    "    n_fs={key:len(new_features_sets[key]) for key in choose_models}\n",
    ")\n",
    "save_image(\n",
    "    saveReport=saveReport, output_directory=output_directory, \n",
    "    img_name=\"test_best_models_roc_curves\", img_ext=img_ext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# key\n",
    "\n",
    "# roc_auc\n",
    "\n",
    "# y_proba\n",
    "\n",
    "# softmax = np.exp(y_proba)/np.sum(np.exp(y_proba))\n",
    "\n",
    "# softmax\n",
    "\n",
    "# fpr_sm, tpr_sm, thresholds_sm = roc_curve(y_test, softmax)\n",
    "\n",
    "# roc_auc_sm = auc(fpr_sm, tpr_sm)\n",
    "\n",
    "# roc_auc_sm\n",
    "\n",
    "# clf = CalibratedClassifierCV(base_estimator=model, cv='prefit')\n",
    "# clf.fit(X_test, y_test)\n",
    "# y_proba_new = clf.predict_proba(X_test)\n",
    "\n",
    "# y_proba_new\n",
    "\n",
    "# fpr_new, tpr_new, thresholds_new = roc_curve(y_test, y_proba_new[:,1])\n",
    "\n",
    "# roc_auc_new = auc(fpr_new, tpr_new)\n",
    "\n",
    "# roc_auc_new\n",
    "\n",
    "# key = 'genepanel'\n",
    "# saveReport = False\n",
    "\n",
    "\n",
    "# fset = list(new_features_sets[key])\n",
    "# X_train = data_train.loc[:,fset].copy()\n",
    "# X_test = data_test.loc[:,fset].copy()\n",
    "\n",
    "# # train model\n",
    "# model, all_coefs, y_train_predictions, y_train_scores, fprs, tprs, interps, aucs = \\\n",
    "#     _run_classification(\n",
    "#         X_train, y_train, **classification_args)\n",
    "\n",
    "# # Test the model\n",
    "# y_test_score = model.score(X_test, y_test)\n",
    "# y_test_predictions = model.predict(X_test)\n",
    "# y_test_predictions = pd.Series(y_test_predictions, index=X_test.index)\n",
    "# y_test_predictions.name = 'test_predictions'\n",
    "\n",
    "\n",
    "# # prepare for the ROC curves on each feature set\n",
    "\n",
    "# y_proba = model.decision_function(X_test)\n",
    "# # Compute ROC curve and area the curve\n",
    "# fpr, tpr, thresholds = roc_curve(y_test, y_proba)\n",
    "# fs_fprs[key] = fpr\n",
    "# fs_tprs[key] = tpr\n",
    "# roc_auc = auc(fpr, tpr)\n",
    "# fs_aucs[key] = roc_auc\n",
    "\n",
    "# roc_auc\n",
    "\n",
    "# y_proba\n",
    "\n",
    "# softmax = np.exp(y_proba)/np.sum(np.exp(y_proba))\n",
    "# softmax\n",
    "\n",
    "# fpr_sm, tpr_sm, thresholds_sm = roc_curve(y_test, softmax)\n",
    "# roc_auc_sm = auc(fpr_sm, tpr_sm)\n",
    "# roc_auc_sm\n",
    "\n",
    "# clf = CalibratedClassifierCV(base_estimator=model, cv='prefit')\n",
    "# clf.fit(X_test, y_test)\n",
    "# y_proba_clf = clf.predict_proba(X_test)\n",
    "# fpr_clf, tpr_clf, thresholds_clf = roc_curve(y_test, y_proba_clf[:, 1])\n",
    "# roc_auc_clf = auc(fpr_clf, tpr_clf)\n",
    "# roc_auc_clf\n",
    "\n",
    "# y_proba_clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
